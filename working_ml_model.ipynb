{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a99bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from faker import Faker\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba588e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the 100000 sample dataset\n",
    "def generate_fake_claims(n=10000):\n",
    "    fake = Faker()\n",
    "    data = []\n",
    "    \n",
    "    for _ in range(n):\n",
    "        admit_date = fake.date_between(start_date='-2y', end_date='-1y')\n",
    "        discharge_date = admit_date + datetime.timedelta(days=random.randint(1, 10))\n",
    "        entry_date = discharge_date + datetime.timedelta(days=random.randint(1, 15))\n",
    "        update_date = entry_date + datetime.timedelta(days=random.randint(1, 15))\n",
    "        \n",
    "        trans_amount = round(random.uniform(1000, 10000), 2)\n",
    "        pay_amount = round(trans_amount * random.uniform(0.3, 0.9), 2)\n",
    "        rev_amount = trans_amount - pay_amount\n",
    "        delay = (update_date - entry_date).days > 10\n",
    "        notes = fake.sentence(nb_words=10) + (\" missing documents\" if delay else \" complete file\")\n",
    "\n",
    "        data.append({\n",
    "            \"AccountType\": random.choice([\"Inpatient\", \"Outpatient\"]),\n",
    "            \"CurrentPrimaryInsurance\": random.choice([\"Insurer A\", \"Insurer B\", \"Insurer C\"]),\n",
    "            \"CurrentFinancialClass\": random.choice([\"Private\", \"Govt\", \"Self-pay\"]),\n",
    "            \"TransactionInsurance\": random.choice([\"Yes\", \"No\"]),\n",
    "            \"BillType\": random.choice([\"Type A\", \"Type B\", \"Type C\"]),\n",
    "            \"ClaimDeptResponsible\": random.choice([\"Cardiology\", \"Radiology\", \"Billing\"]),\n",
    "            \"TransAmount\": trans_amount,\n",
    "            \"PAYAmount\": pay_amount,\n",
    "            \"REVAmount\": rev_amount,\n",
    "            \"Notes\": notes,\n",
    "            \"IsDelayed\": int(delay)\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = generate_fake_claims(100000)\n",
    "df.head()\n",
    "df[\"Notes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef867d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the feature are the input data to the ml models\n",
    "\n",
    "features = [\n",
    "    \"AccountType\", \"CurrentPrimaryInsurance\", \"CurrentFinancialClass\",\n",
    "    \"TransactionInsurance\", \"BillType\", \"ClaimDeptResponsible\",\n",
    "    \"TransAmount\", \"PAYAmount\", \"REVAmount\", \"Notes\"\n",
    "]\n",
    "\n",
    "X = df[features]  # Features\n",
    "\n",
    "# the output weather it was delay are not\n",
    "y = df[\"IsDelayed\"]  # Target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a886e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columntransformer this will trainformthe colum to require colum type\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    # stabdardsclare it was represending the numerical values\n",
    "    (\"num\", StandardScaler(), [\"TransAmount\", \"PAYAmount\", \"REVAmount\"]),\n",
    "    # when ever we use like option for the colums we can use the onwhorencoder\n",
    "    (\"cat\", OneHotEncoder(), [\n",
    "        \"AccountType\", \"CurrentPrimaryInsurance\", \"CurrentFinancialClass\",\n",
    "        \"TransactionInsurance\", \"BillType\", \"ClaimDeptResponsible\"\n",
    "    ]),\n",
    "    #it will turn the nodes into the numicraical values\n",
    "    (\"txt\", TfidfVectorizer(max_features=50), \"Notes\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f0fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and Train Models\n",
    "\n",
    "# in the pipline we cotain the two main thing preproeceeor it will contain the streucture coilum we process the data in step 4 and classider contain the model to tarin\n",
    "\n",
    "xgb_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42))\n",
    "])\n",
    "\n",
    "# it will contain the 80% of the data\n",
    "xgb_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49439d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for randome forret method\n",
    "\n",
    "rf_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "rf_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c919f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for  logical regression method\n",
    "\n",
    "log_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", LogisticRegression(max_iter=500))\n",
    "])\n",
    "log_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d266e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"XGBoost\": xgb_model,\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"Logistic Regression\": log_model\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n==== {name} ====\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54879cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"On-Time\", \"Delayed\"])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"XGBoost - Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcaa696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Saving the trained model (XGBoost, Random Forest, etc.)\n",
    "joblib.dump(xgb_model, 'sample_model_xgboost.pkl')  # You can name your file whatever you want\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
